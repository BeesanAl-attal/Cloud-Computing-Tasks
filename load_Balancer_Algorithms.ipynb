{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tALjzQGTIWk4"
      },
      "source": [
        "#Problem Explanation\n",
        "\n",
        "*  First what is load balancing algorithm ?\n",
        "  It is a set of rules that a load balancer follows to determine the best server for each of the different client requests. (AWS)\n",
        "\n",
        "  * So here we are trying to write down the \"set of rules\" using python, and we need to test it at least on 2 devices, since obviously we are trying to distribute the request across two or multiple servers (devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ubNWHg9HIOH8"
      },
      "outputs": [],
      "source": [
        "# importing the dependencies\n",
        "import tensorflow as tf  # To simulate heavy computational tasks (Matrix multiplication)\n",
        "import numpy as np  # math purposes\n",
        "import time # In our case (Measuring execution time: Track how long a code block takes to run)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MWDnrwgoIhEn"
      },
      "outputs": [],
      "source": [
        "# Fucntion to simulate heavy computation --> Matrix multiplication\n",
        "def compute_on_device(device_name):\n",
        "\n",
        "  with tf.device(device_name):\n",
        "\n",
        "\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Create a latge random matrix\n",
        "      matrix_size = 1000\n",
        "      a = tf.random.normal([matrix_size, matrix_size])\n",
        "      b = tf.random.normal([matrix_size, matrix_size])\n",
        "\n",
        "      result = tf.matmul(a, b)\n",
        "      end_time = time.time()\n",
        "\n",
        "      return (end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TgiPIlZJ90M"
      },
      "source": [
        "# Least Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW8M7SyuJ9lv",
        "outputId": "037dc918-4b0c-4ae1-d722-4b44bb428dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1 executed on /GPU:1, Time: 0.0735 seconds\n",
            "Task 2 executed on /GPU:1, Time: 0.0718 seconds\n",
            "Task 3 executed on /CPU:0, Time: 0.0664 seconds\n",
            "Task 4 executed on /GPU:1, Time: 0.0684 seconds\n",
            "Task 5 executed on /CPU:0, Time: 0.0687 seconds\n",
            "Task 6 executed on /GPU:1, Time: 0.0755 seconds\n",
            "Task 7 executed on /CPU:0, Time: 0.0757 seconds\n",
            "Task 8 executed on /GPU:1, Time: 0.0695 seconds\n",
            "Task 9 executed on /CPU:0, Time: 0.0722 seconds\n",
            "Task 10 executed on /GPU:1, Time: 0.0721 seconds\n"
          ]
        }
      ],
      "source": [
        "# here i assumed that the cpu has two connections\n",
        "# so when i called the function the tasks were assigned to the gpu since it has the least connections\n",
        "# after a couple of tasks the gpu will have more connection thus, the cpu will be assigned more tasks\n",
        "# note that in this code there is no maximum connections specified for each device, because i wanted to foucus on the main concept of the algorithm\n",
        "\n",
        "devices = {\n",
        "    \"/CPU:0\": 2,  # Current connections\n",
        "    \"/GPU:1\": 0,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def least_connections(devices, num_tasks):\n",
        "  for i in range(num_tasks):\n",
        "\n",
        "    # find the device with the least ammount connections\n",
        "    # minimum \"value\" is our key\n",
        "    device = min(devices, key=devices.get)\n",
        "    # conpute on the chosen device from the previous code of line\n",
        "    task_time = compute_on_device(device)\n",
        "\n",
        "    # updation statement +1 connections each time a task is \"computed\"\n",
        "    devices[device] += 1 # remeber its a dict\n",
        "    print(f\"Task {i + 1} executed on {device}, Time: {task_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# call the fucntion\n",
        "least_connections(devices, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzd6a7MRIkUH"
      },
      "source": [
        "# Weighted Round robin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hc7aaTMIoSL",
        "outputId": "7bdbbc59-ae9c-411c-9f83-7e26c91f37ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1 executed on /CPU:0, Time: 0.0719 seconds\n",
            "Task 2 executed on /CPU:0, Time: 0.0679 seconds\n",
            "Task 3 executed on /CPU:0, Time: 0.0696 seconds\n",
            "Task 4 executed on /GPU:1, Time: 0.0728 seconds\n",
            "Task 5 executed on /GPU:1, Time: 0.0712 seconds\n",
            "Task 6 executed on /GPU:1, Time: 0.0666 seconds\n",
            "Task 7 executed on /GPU:1, Time: 0.0721 seconds\n",
            "Task 8 executed on /CPU:0, Time: 0.0688 seconds\n",
            "Task 9 executed on /CPU:0, Time: 0.0667 seconds\n",
            "Task 10 executed on /CPU:0, Time: 0.0717 seconds\n"
          ]
        }
      ],
      "source": [
        "devices = {\"/CPU:0\": 3, \"/GPU:1\": 4} # here we used the dict data structure in order to \"link\" the device with its weight\n",
        "\n",
        "def weighted_round_robin(devices, num_tasks):\n",
        "\n",
        "    # this code snippet is used to create a lists where each device appears multiple times == its weight\n",
        "    device_list = []\n",
        "    for device, weight in devices.items():\n",
        "        device_list.extend([device] * weight)\n",
        "\n",
        "    # Loop to distribute tasks, here the tasks will be distributed first to the cpu since its first in the line\n",
        "    for i in range(num_tasks):\n",
        "        device = device_list[i % len(device_list)]  # Select device based on the current index\n",
        "\n",
        "        # Simulate task computation on the selected device\n",
        "        task_time = compute_on_device(device)\n",
        "        print(f\"Task {i + 1} executed on {device}, Time: {task_time:.4f} seconds\")\n",
        "\n",
        "# Calling the fucntion\n",
        "weighted_round_robin(devices, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vha1KCDWPBre"
      },
      "source": [
        "# IP Hash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toj0d8ZCPGHe",
        "outputId": "a9ab9ce6-028a-4076-b9ea-5d2dd7223b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request 1 from 192.168.1.10 executed on /GPU:1, Time: 0.0700 seconds\n",
            "Request 2 from 192.168.1.11 executed on /GPU:1, Time: 0.0697 seconds\n",
            "Request 3 from 192.168.1.10 executed on /GPU:1, Time: 0.0671 seconds\n",
            "Request 4 from 192.168.1.12 executed on /CPU:1, Time: 0.0690 seconds\n",
            "Request 5 from 192.168.1.11 executed on /GPU:1, Time: 0.0737 seconds\n"
          ]
        }
      ],
      "source": [
        "# This algorithm is concered with privacy and security as it appers for its name\n",
        "# here we want to map the task to the server based on the ip address of the client that requested the task\n",
        "devices  = [\"/CPU:0\", \"/GPU:1\", \"/CPU:1\"]\n",
        "\n",
        "import hashlib\n",
        "\n",
        "# this fucntion is to hash the ip address using md5 encryption\n",
        "def ip_hash(client_ip, servers):\n",
        "\n",
        "  # calculate the hash of the clients ip\n",
        "  hash_value = int(hashlib.md5(client_ip.encode()).hexdigest(), 16)\n",
        "  # this line servers to determine which server to assign a request based on the hash value that is calculated\n",
        "  # (hash-based selection)\n",
        "  server_index = hash_value % len(servers)\n",
        "\n",
        "  return servers[server_index] # here is the selection\n",
        "\n",
        "def process_requests(client_ips):\n",
        "  # note: enumerate(list) generates pairs of (index, item) for each item in the  list.\n",
        "  for i, client_ip in enumerate(client_ips): # here the i will iterate for the index and the client_ip will iterate for the item :), very nice actually\n",
        "\n",
        "    server = ip_hash(client_ip, devices)\n",
        "\n",
        "    task_time = compute_on_device(server)\n",
        "    print(f\"Request {i + 1} from {client_ip} executed on {server}, Time: {task_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "# the IPs where generated using chat gpt\n",
        "client_ips = [\"192.168.1.10\", \"192.168.1.11\", \"192.168.1.10\", \"192.168.1.12\", \"192.168.1.11\"]\n",
        "process_requests(client_ips)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fimugAOSdCC"
      },
      "source": [
        "# Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ_g9hmuSf4Y"
      },
      "source": [
        "In this notebook i chose to work on the following load balancing algorithms: Least connections, Weighted Round Robin, IP has.\n",
        "# Brief explanation for each algorithm\n",
        "1. The least connections\n",
        "is a simple algorithm that tracks the number of connections a server has with a client and  chooses the server to use by prioritizing the device with the least connections as its name implies, to check the results [Least Connections](https://colab.research.google.com/drive/1XPbVQ0UNutsKc49p-kWI24q6SLDC2PLO#scrollTo=IW8M7SyuJ9lv&line=1&uniqifier=1)\n",
        "\n",
        "2. Weighted Round Robin\n",
        " The algorithm is based on the assigned weights for each device which is the maximum load allowed for each device and to check the results  [Weighted Round Robin](https://colab.research.google.com/drive/1XPbVQ0UNutsKc49p-kWI24q6SLDC2PLO#scrollTo=6hc7aaTMIoSL&line=1&uniqifier=1)\n",
        "\n",
        " 3. IP hash\n",
        " as the name implies this algorithm is concerned with privacy and encryption in addition to its main purpose (tasks distribution), here the tasks are mapped to each server based on the ip address of the client that requested the task, and the ip address is hashed, you can check the results  [IP Hash](https://colab.research.google.com/drive/1XPbVQ0UNutsKc49p-kWI24q6SLDC2PLO#scrollTo=toj0d8ZCPGHe&line=1&uniqifier=1)\n",
        "\n",
        "Finally, Comparison of performance\n",
        "1. least connections --> based on the results; responsiveness and efficiency but may lead to uneven load if one device quickly fills up.\n",
        "gpu on average --> 0.0723 seconds\n",
        "cpu pn average --> 0.0714 seconds\n",
        "execution time --> gpu > cpu\n",
        "\n",
        "2. Weighted Round Robin  --> More consistent than the Least Connections method, but the CPU can become a bottleneck with increased load.\n",
        "gpu on average --> 0.0707 seconds\n",
        "cpu pn average --> 0.0696 seconds\n",
        "execution time --> gpu > cpu\n",
        "\n",
        "3. IP hash --> based on the  distribution of client IPs; might lead to inefficiencies if certain servers are overwhelmed by repeated requests from a small number of clients.\n",
        "gpu on average --> 0.0701 seconds\n",
        "cpu pn average -->0.0690 seconds only one task was performed on the cpu\n",
        "execution time --> gpu > cpu\n",
        "\n",
        "surprisingly the cpu performs faster gpu in our case\n",
        "I think the reason behind that is the gpu where assigned more tasks so the average execution time was higher\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
